{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# Initialize Spark\n",
    "appName = \"Project - Machine Learning Techniques on MQTT\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# You need to create SQL Context to conduct some database operations like what we will\n",
    "sqlContext = SQLContext(sc)\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\nickc\\OneDrive\\Documents\\GitHub\\course-project-option-2-nick1117\\DataFolder\\archive\\Data\\FINAL_CSV\\train70_augmented.csv\"\n",
    "train_data = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "\n",
    "test_path = r\"C:\\Users\\nickc\\OneDrive\\Documents\\GitHub\\course-project-option-2-nick1117\\DataFolder\\archive\\Data\\FINAL_CSV\\test30_augmented.csv\"\n",
    "test_data = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "train_df = train_data.withColumn(\"dataset_type\", lit(\"train\"))\n",
    "test_df = test_data.withColumn(\"dataset_type\", lit(\"test\"))\n",
    "df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"tcp_flags\",\n",
    "    \"tcp_time_delta\",\n",
    "    \"tcp_length\",\n",
    "    \"mqtt_connack_flags\",\n",
    "    \"mqtt_connack_reserved_flags\",\n",
    "    \"mqtt_connack_session_present\",\n",
    "    \"mqtt_connack_return_code\",\n",
    "    \"mqtt_connect_clean_session_flag\",\n",
    "    \"mqtt_connect_password_flag\",\n",
    "    \"mqtt_connect_qos_level\",\n",
    "    \"mqtt_connect_reserved_flag\",\n",
    "    \"mqtt_connect_retain_flag\",\n",
    "    \"mqtt_connect_username_flag\",\n",
    "    \"mqtt_connect_will_flag\",\n",
    "    \"mqtt_connect_flags\",\n",
    "    \"mqtt_duplicate_flag\",\n",
    "    \"mqtt_header_flags\",\n",
    "    \"mqtt_keep_alive_interval\",\n",
    "    \"mqtt_length\",\n",
    "    \"mqtt_message\",\n",
    "    \"mqtt_message_id\",\n",
    "    \"mqtt_message_type\",\n",
    "    \"mqtt_protocol_length\",\n",
    "    \"mqtt_protocol_name\",\n",
    "    \"mqtt_qos_level\",\n",
    "    \"mqtt_retain_flag\",\n",
    "    \"mqtt_subscription_qos_level\",\n",
    "    \"mqtt_subscription_ack_qos_level\",\n",
    "    \"mqtt_version\",\n",
    "    \"mqtt_will_message\",\n",
    "    \"mqtt_will_message_length\",\n",
    "    \"mqtt_will_topic\",\n",
    "    \"mqtt_will_topic_length\",\n",
    "    \"target_class\",\n",
    "    \"dataset_type\"\n",
    "]\n",
    "\n",
    "df = df.toDF(*new_column_names)\n",
    "\n",
    "train_df = train_df.toDF(*new_column_names)\n",
    "test_df = test_df.toDF(*new_column_names)\n",
    "train_df = train_df.drop(\"dataset_type\")\n",
    "test_df = test_df.drop(\"dataset_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "col_names = new_column_names\n",
    "    # Define the columns\n",
    "binary_cols = [\n",
    "    \"mqtt_connack_reserved_flags\",\n",
    "    \"mqtt_connack_session_present\",\n",
    "    \"mqtt_connect_clean_session_flag\",\n",
    "    \"mqtt_connect_password_flag\",\n",
    "    \"mqtt_connect_reserved_flag\",\n",
    "    \"mqtt_connect_retain_flag\",\n",
    "    \"mqtt_connect_username_flag\",\n",
    "    \"mqtt_connect_will_flag\",\n",
    "    \"mqtt_duplicate_flag\",\n",
    "    \"mqtt_retain_flag\",\n",
    "    \"mqtt_connect_qos_level\"\n",
    "]\n",
    "\n",
    "nominal_cols = [\n",
    "    \"tcp_flags\",\n",
    "    \"mqtt_connack_flags\",\n",
    "    \"mqtt_connack_return_code\",\n",
    "    \"mqtt_connect_flags\",\n",
    "    \"mqtt_header_flags\",\n",
    "    \"mqtt_message_type\",\n",
    "    \"mqtt_qos_level\",\n",
    "    \"mqtt_version\",\n",
    "    \"mqtt_message\",\n",
    "    'mqtt_protocol_name'\n",
    "]\n",
    "\n",
    "continuous_cols = [\n",
    "    \"tcp_time_delta\",\n",
    "    \"tcp_length\",\n",
    "    \"mqtt_keep_alive_interval\",\n",
    "    \"mqtt_length\",\n",
    "    \"mqtt_protocol_length\",\n",
    "    \"mqtt_will_message_length\",\n",
    "    \"mqtt_will_topic_length\",\n",
    "    \"mqtt_will_topic\",\n",
    "    \"mqtt_will_message\",\n",
    "    \"mqtt_subscription_ack_qos_level\",\n",
    "    \"mqtt_subscription_qos_level\",\n",
    "    \"mqtt_message_id\"\n",
    "]\n",
    "\n",
    "class OutcomeCreater(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_multiclass = udf(lambda name:\n",
    "                                  0.0 if name == 'legitimate' else\n",
    "                                  1.0 if name == 'slowite' else\n",
    "                                  2.0 if name == 'bruteforce' else\n",
    "                                  3.0 if name == 'flood' else\n",
    "                                  4.0 if name == 'malformed' else\n",
    "                                  5.0 if name == 'dos' else\n",
    "                                  -1.0, DoubleType())\n",
    "        output_df = dataset.withColumn('outcome', label_to_multiclass(col('target_class')))\n",
    "        output_df = output_df.drop(\"target_class\")\n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class FeatureTypeCaster(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer):\n",
    "    def __init__(self, columns_to_drop=None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Columns to remove\n",
    "    corelated_cols_to_remove = [\"mqtt_connack_reserved_flags\", \"mqtt_connack_session_present\", \"mqtt_connect_qos_level\",\n",
    "                                \"mqtt_connect_reserved_flag\", \"mqtt_connect_retain_flag\", \"mqtt_connect_will_flag\", \"mqtt_message\", \n",
    "                                \"mqtt_subscription_qos_level\", \"mqtt_subscription_ack_qos_level\", \"mqtt_will_message\",\n",
    "                                \"mqtt_will_message_length\", \"mqtt_will_topic\", \"mqtt_will_topic_length\",'mqtt_protocol_name']\n",
    "\n",
    "    # Stage where columns are cast as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Convert nominal columns to string type\n",
    "    class NominalTypeCaster(Transformer):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def _transform(self, dataset):\n",
    "            output_df = dataset\n",
    "            for col_name in nominal_cols:\n",
    "                output_df = output_df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
    "            return output_df\n",
    "\n",
    "    stage_nominal_typecaster = NominalTypeCaster()\n",
    "\n",
    "    # Create a list of StringIndexers with handleInvalid='keep'\n",
    "    stage_nominal_indexers = [\n",
    "        StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') \n",
    "        for col in nominal_cols\n",
    "    ]\n",
    "\n",
    "    # Create a list of OneHotEncoders with handleInvalid='keep'\n",
    "    stage_nominal_onehot_encoders = [\n",
    "        OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid='keep') \n",
    "        for col in nominal_cols\n",
    "    ]\n",
    "\n",
    "    # Feature columns assembly\n",
    "    feature_cols = continuous_cols + binary_cols + [col + \"_encoded\" for col in nominal_cols]\n",
    "\n",
    "    # Remove correlated columns from features\n",
    "    for col_name in corelated_cols_to_remove:\n",
    "        if col_name in feature_cols:\n",
    "            feature_cols.remove(col_name)\n",
    "\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the features\n",
    "    stage_scaler = StandardScaler(inputCol='vectorized_features', outputCol='features')\n",
    "\n",
    "    # Stage for creating the outcome column\n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing unnecessary columns\n",
    "    columns_to_drop = (\n",
    "        nominal_cols + \n",
    "        [col + \"_index\" for col in nominal_cols] + \n",
    "        [col + \"_encoded\" for col in nominal_cols] + \n",
    "        binary_cols + continuous_cols + ['vectorized_features']\n",
    "    )\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop=columns_to_drop)\n",
    "\n",
    "    # Connect the stages into a pipeline\n",
    "    stages = (\n",
    "        [stage_typecaster, stage_nominal_typecaster] + \n",
    "        stage_nominal_indexers + \n",
    "        stage_nominal_onehot_encoders + \n",
    "        [stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper]\n",
    "    )\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing pipeline\n",
    "pipeline = get_preprocess_pipeline()\n",
    "\n",
    "# Fit the pipeline to data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transform the data\n",
    "train_df_preprocessed = pipeline_model.transform(train_df)\n",
    "test_df_preprocessed = pipeline_model.transform(test_df)\n",
    "\n",
    "# Cache the dataframes\n",
    "train_df_final = train_df_preprocessed.cache()\n",
    "test_df_final = test_df_preprocessed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline, Transformer\n",
    "# from pyspark.sql.functions import col, udf, when\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "# from pyspark.sql import DataFrame\n",
    "# from functools import reduce\n",
    "\n",
    "# # Assuming 'binary_cols', 'continuous_cols', and 'nominal_cols' are already defined\n",
    "\n",
    "# # Outlier detection function\n",
    "# def column_add(a, b):\n",
    "#     return a.__add__(b)\n",
    "\n",
    "# def find_outliers(df):\n",
    "#     # Identifying the numerical columns in a spark dataframe\n",
    "#     numeric_columns = [column[0] for column in df.dtypes if column[1] in ('int', 'double')]\n",
    "\n",
    "#     # Using the for loop to create new columns by identifying the outliers for each feature\n",
    "#     for column_name in numeric_columns:\n",
    "#         Q1 = df.approxQuantile(column_name, [0.25], relativeError=0)\n",
    "#         Q3 = df.approxQuantile(column_name, [0.75], relativeError=0)\n",
    "        \n",
    "#         # IQR : Inter Quartile Range\n",
    "#         IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "#         # Define thresholds for outliers\n",
    "#         lower_bound = Q1[0] - 1.5 * IQR\n",
    "#         upper_bound = Q3[0] + 1.5 * IQR\n",
    "        \n",
    "#         isOutlierCol = 'is_outlier_{}'.format(column_name)\n",
    "        \n",
    "#         df = df.withColumn(isOutlierCol, when((df[column_name] < lower_bound) | (df[column_name] > upper_bound), 1).otherwise(0))\n",
    "\n",
    "#     # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "#     selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "#     # Adding all the outlier columns into a new column \"total_outliers\", to see the total number of outliers\n",
    "#     df = df.withColumn('total_outliers', reduce(column_add, (df[col] for col in selected_columns)))\n",
    "\n",
    "#     # Dropping the extra columns created above, just to create nice dataframe without extra columns\n",
    "#     df = df.drop(*selected_columns)\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# col_names = new_column_names\n",
    "#     # Define the columns\n",
    "# binary_cols = [\n",
    "#     \"mqtt_connack_reserved_flags\",\n",
    "#     \"mqtt_connack_session_present\",\n",
    "#     \"mqtt_connect_clean_session_flag\",\n",
    "#     \"mqtt_connect_password_flag\",\n",
    "#     \"mqtt_connect_reserved_flag\",\n",
    "#     \"mqtt_connect_retain_flag\",\n",
    "#     \"mqtt_connect_username_flag\",\n",
    "#     \"mqtt_connect_will_flag\",\n",
    "#     \"mqtt_duplicate_flag\",\n",
    "#     \"mqtt_retain_flag\",\n",
    "#     \"mqtt_connect_qos_level\"\n",
    "# ]\n",
    "\n",
    "# nominal_cols = [\n",
    "#     \"tcp_flags\",\n",
    "#     \"mqtt_connack_flags\",\n",
    "#     \"mqtt_connack_return_code\",\n",
    "#     \"mqtt_connect_flags\",\n",
    "#     \"mqtt_header_flags\",\n",
    "#     \"mqtt_message_type\",\n",
    "#     \"mqtt_qos_level\",\n",
    "#     \"mqtt_version\",\n",
    "#     \"mqtt_message\",\n",
    "#     'mqtt_protocol_name'\n",
    "# ]\n",
    "\n",
    "# continuous_cols = [\n",
    "#     \"tcp_time_delta\",\n",
    "#     \"tcp_length\",\n",
    "#     \"mqtt_keep_alive_interval\",\n",
    "#     \"mqtt_length\",\n",
    "#     \"mqtt_protocol_length\",\n",
    "#     \"mqtt_will_message_length\",\n",
    "#     \"mqtt_will_topic_length\",\n",
    "#     \"mqtt_will_topic\",\n",
    "#     \"mqtt_will_message\",\n",
    "#     \"mqtt_subscription_ack_qos_level\",\n",
    "#     \"mqtt_subscription_qos_level\",\n",
    "#     \"mqtt_message_id\"\n",
    "# ]\n",
    "\n",
    "# # Custom Transformers\n",
    "# class OutcomeCreater(Transformer):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         label_to_multiclass = udf(lambda name:\n",
    "#                                   0.0 if name == 'legitimate' else\n",
    "#                                   1.0 if name == 'slowite' else\n",
    "#                                   2.0 if name == 'bruteforce' else\n",
    "#                                   3.0 if name == 'flood' else\n",
    "#                                   4.0 if name == 'malformed' else\n",
    "#                                   5.0 if name == 'dos' else\n",
    "#                                   -1.0, DoubleType())\n",
    "#         output_df = dataset.withColumn('outcome', label_to_multiclass(col('target_class')))\n",
    "#         output_df = output_df.drop(\"target_class\")\n",
    "#         output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "#         return output_df\n",
    "\n",
    "# class FeatureTypeCaster(Transformer):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _transform(self, dataset: DataFrame):\n",
    "#         output_df = dataset\n",
    "#         for col_name in binary_cols + continuous_cols:\n",
    "#             output_df = output_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "#         return output_df\n",
    "\n",
    "# class NominalTypeCaster(Transformer):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         output_df = dataset\n",
    "#         for col_name in nominal_cols:\n",
    "#             output_df = output_df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
    "#         return output_df\n",
    "\n",
    "# class OutlierHandler(Transformer):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         return find_outliers(dataset)\n",
    "\n",
    "# class ColumnDropper(Transformer):\n",
    "#     def __init__(self, columns_to_drop=None):\n",
    "#         super().__init__()\n",
    "#         self.columns_to_drop = columns_to_drop\n",
    "\n",
    "#     def _transform(self, dataset):\n",
    "#         return dataset.drop(*self.columns_to_drop)\n",
    "\n",
    "# def get_preprocess_pipeline():\n",
    "#     # Columns to remove\n",
    "#     corelated_cols_to_remove = [\n",
    "#         \"mqtt_connack_reserved_flags\", \"mqtt_connack_session_present\", \"mqtt_connect_qos_level\",\n",
    "#         \"mqtt_connect_reserved_flag\", \"mqtt_connect_retain_flag\", \"mqtt_connect_will_flag\", \"mqtt_message\",\n",
    "#         \"mqtt_subscription_qos_level\", \"mqtt_subscription_ack_qos_level\", \"mqtt_will_message\",\n",
    "#         \"mqtt_will_message_length\", \"mqtt_will_topic\", \"mqtt_will_topic_length\", 'mqtt_protocol_name'\n",
    "#     ]\n",
    "\n",
    "#     # Stage where columns are cast as appropriate types\n",
    "#     stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "#     # Convert nominal columns to string type\n",
    "#     stage_nominal_typecaster = NominalTypeCaster()\n",
    "\n",
    "#     # Stage for handling outliers\n",
    "#     stage_outlier_handler = OutlierHandler()\n",
    "\n",
    "#     # Create a list of StringIndexers with handleInvalid='keep'\n",
    "#     stage_nominal_indexers = [\n",
    "#         StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep')\n",
    "#         for col in nominal_cols\n",
    "#     ]\n",
    "\n",
    "#     # Create a list of OneHotEncoders with handleInvalid='keep'\n",
    "#     stage_nominal_onehot_encoders = [\n",
    "#         OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\", handleInvalid='keep')\n",
    "#         for col in nominal_cols\n",
    "#     ]\n",
    "\n",
    "#     # Feature columns assembly\n",
    "#     feature_cols = continuous_cols + binary_cols + [col + \"_encoded\" for col in nominal_cols]\n",
    "\n",
    "#     # Remove correlated columns from features\n",
    "#     for col_name in corelated_cols_to_remove:\n",
    "#         if col_name in feature_cols:\n",
    "#             feature_cols.remove(col_name)\n",
    "\n",
    "#     stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "#     # Stage where we scale the features\n",
    "#     stage_scaler = StandardScaler(inputCol='vectorized_features', outputCol='features')\n",
    "\n",
    "#     # Stage for creating the outcome column\n",
    "#     stage_outcome = OutcomeCreater()\n",
    "\n",
    "#     # Removing unnecessary columns\n",
    "#     # Exclude 'total_outliers' from columns_to_drop to keep it after the pipeline runs\n",
    "#     columns_to_drop = (\n",
    "#         nominal_cols +\n",
    "#         [col + \"_index\" for col in nominal_cols] +\n",
    "#         [col + \"_encoded\" for col in nominal_cols] +\n",
    "#         binary_cols + continuous_cols + ['vectorized_features']\n",
    "#         # 'total_outliers' is not included here\n",
    "#     )\n",
    "#     stage_column_dropper = ColumnDropper(columns_to_drop=columns_to_drop)\n",
    "\n",
    "#     # Connect the stages into a pipeline\n",
    "#     stages = (\n",
    "#         [stage_typecaster, stage_nominal_typecaster, stage_outlier_handler] +\n",
    "#         stage_nominal_indexers +\n",
    "#         stage_nominal_onehot_encoders +\n",
    "#         [stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper]\n",
    "#     )\n",
    "#     pipeline = Pipeline(stages=stages)\n",
    "\n",
    "#     return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Fit the pipeline to training data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pipeline_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Transform the data\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\ml\\pipeline.py:132\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    131\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m--> 132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 133\u001b[0m, in \u001b[0;36mOutlierHandler._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mfind_outliers\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m numeric_columns:\n\u001b[1;32m---> 20\u001b[0m     Q1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelativeError\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     Q3 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapproxQuantile(column_name, [\u001b[38;5;241m0.75\u001b[39m], relativeError\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:4847\u001b[0m, in \u001b[0;36mDataFrame.approxQuantile\u001b[1;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[0;32m   4845\u001b[0m relativeError \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(relativeError)\n\u001b[1;32m-> 4847\u001b[0m jaq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelativeError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4848\u001b[0m jaq_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(j) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jaq]\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# pipeline = get_preprocess_pipeline()\n",
    "\n",
    "# # Fit the pipeline to training data\n",
    "# pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# # Transform the data\n",
    "# train_df_preprocessed = pipeline_model.transform(train_df)\n",
    "# test_df_preprocessed = pipeline_model.transform(test_df)\n",
    "\n",
    "# # Cache the dataframes if needed\n",
    "# train_df_preprocessed = train_df_preprocessed.cache()\n",
    "# test_df_preprocessed = test_df_preprocessed.cache()\n",
    "\n",
    "# # Display the first few rows of the transformed training DataFrame\n",
    "# print(\"Training Data Before Filtering:\")\n",
    "# train_df_preprocessed.show(n=10, truncate=False)\n",
    "\n",
    "# # Now, filter out rows with 4 or more outliers\n",
    "# threshold = 4\n",
    "# train_df_filtered = train_df_preprocessed.filter(col('total_outliers') < threshold)\n",
    "# test_df_filtered = test_df_preprocessed.filter(col('total_outliers') < threshold)\n",
    "\n",
    "# # Display the first few rows of the filtered training DataFrame\n",
    "# print(\"Training Data After Filtering:\")\n",
    "# train_df_filtered.show(n=10, truncate=False)\n",
    "\n",
    "# # Optionally, drop the 'total_outliers' column if it's no longer needed\n",
    "# train_df_final = train_df_filtered.drop('total_outliers')\n",
    "# test_df_final = test_df_filtered.drop('total_outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_outlier_handling = find_outliers(train_df_preprocessed)\n",
    "train_df_with_outlier_handling.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_substituted_na_and_outliers = train_df_with_outlier_handling.\\\n",
    "        filter(train_df_with_outlier_handling['total_Outliers']<=4)\n",
    "print(train_df_with_substituted_na_and_outliers.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert the 'features' vector column to an array and expand each element as a separate column\n",
    "train_df_expanded = train_df_with_substituted_na_and_outliers.withColumn(\n",
    "    \"features_array\", vector_to_array(col(\"features\"))\n",
    ")\n",
    "\n",
    "# Determine the number of features (dimensions) in the vector\n",
    "num_features = len(train_df_expanded.select(\"features_array\").head()[0])\n",
    "\n",
    "# Expand each element in the array to its own column\n",
    "for i in range(num_features):\n",
    "    train_df_expanded = train_df_expanded.withColumn(f\"feature_{i}\", col(\"features_array\")[i])\n",
    "\n",
    "# Drop the original 'features' and 'features_array' columns\n",
    "numeric_df = train_df_expanded.drop(\"features\", \"features_array\").select(\n",
    "    *[f\"feature_{i}\" for i in range(num_features)], \"outcome\", \"total_outliers\"\n",
    ")\n",
    "\n",
    "# Convert to Pandas and calculate the correlation matrix\n",
    "correlation_matrix = numeric_df.toPandas().corr()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_df_with_substituted_na_and_outliers.toPandas().corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_outlier_handling = find_outliers(train_df_preprocessed)\n",
    "\n",
    "# Count rows with 4 or fewer outliers\n",
    "count_4_or_less_outliers = train_df_with_outlier_handling.filter(train_df_with_outlier_handling['total_outliers'] <= 4).count()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with 4 or fewer outliers: {count_4_or_less_outliers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
