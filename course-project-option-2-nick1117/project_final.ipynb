{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for local device\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# Initialize Spark\n",
    "appName = \"Project - Machine Learning Techniques on MQTT\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# You need to create SQL Context to conduct some database operations like what we will\n",
    "sqlContext = SQLContext(sc)\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[*]\").appName(\"sparktest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Cloud\n",
    "\n",
    "# # import findspark\n",
    "# # findspark.init()\n",
    "# # findspark.find()\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# import requests\n",
    "# #import feedparser\n",
    "# import pyspark\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# # Initialize Spark\n",
    "# appName = \"Project - Machine Learning Techniques on MQTT\"\n",
    "# master = \"yarn\"\n",
    "\n",
    "# # Create Configuration object for Spark.\n",
    "# # conf = pyspark.SparkConf()\\\n",
    "# #     .set('spark.driver.host','127.0.0.1')\\\n",
    "# #     .setAppName(appName)\\\n",
    "# #     .setMaster(master)\n",
    "\n",
    "# conf = pyspark.SparkConf()\\\n",
    "#     .setAppName(appName)\\\n",
    "#     .setMaster(master)\n",
    "# # Create Spark Context with the new configurations rather than relying on the default\n",
    "# sc = SparkContext.getOrCreate(conf=conf)\n",
    "# # You need to create SQL Context to conduct some database operations like what we will\n",
    "# sqlContext = SQLContext(sc)\n",
    "# # If you have SQL context, you create the session from the Spark Context\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"C:\\Users\\nickc\\OneDrive\\Documents\\GitHub\\course-project-option-2-nick1117\\DataFolder\\archive\\Data\\FINAL_CSV\\train70_augmented.csv\"\n",
    "train_data = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "\n",
    "test_path = r\"C:\\Users\\nickc\\OneDrive\\Documents\\GitHub\\course-project-option-2-nick1117\\DataFolder\\archive\\Data\\FINAL_CSV\\test30_augmented.csv\"\n",
    "test_data = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cloud - will need to set up bucket\n",
    "\n",
    "# train_path = 'gs://project-bucket-chermak/DataFolder/archive/Data/FINAL_CSV/train70_augmented.csv'\n",
    "# train_data = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "\n",
    "# test_path = 'gs://project-bucket-chermak/DataFolder/archive/Data/FINAL_CSV/test30_augmented.csv'\n",
    "# test_data = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "train_df = train_data.withColumn(\"dataset_type\", lit(\"train\"))\n",
    "test_df = test_data.withColumn(\"dataset_type\", lit(\"test\"))\n",
    "df = train_df.union(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tcp.flags', 'tcp.time_delta', 'tcp.len', 'mqtt.conack.flags', 'mqtt.conack.flags.reserved', 'mqtt.conack.flags.sp', 'mqtt.conack.val', 'mqtt.conflag.cleansess', 'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved', 'mqtt.conflag.retain', 'mqtt.conflag.uname', 'mqtt.conflag.willflag', 'mqtt.conflags', 'mqtt.dupflag', 'mqtt.hdrflags', 'mqtt.kalive', 'mqtt.len', 'mqtt.msg', 'mqtt.msgid', 'mqtt.msgtype', 'mqtt.proto_len', 'mqtt.protoname', 'mqtt.qos', 'mqtt.retain', 'mqtt.sub.qos', 'mqtt.suback.qos', 'mqtt.ver', 'mqtt.willmsg', 'mqtt.willmsg_len', 'mqtt.willtopic', 'mqtt.willtopic_len', 'target', 'dataset_type']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"tcp_flags\",\n",
    "    \"tcp_time_delta\",\n",
    "    \"tcp_length\",\n",
    "    \"mqtt_connack_flags\",\n",
    "    \"mqtt_connack_reserved_flags\",\n",
    "    \"mqtt_connack_session_present\",\n",
    "    \"mqtt_connack_return_code\",\n",
    "    \"mqtt_connect_clean_session_flag\",\n",
    "    \"mqtt_connect_password_flag\",\n",
    "    \"mqtt_connect_qos_level\",\n",
    "    \"mqtt_connect_reserved_flag\",\n",
    "    \"mqtt_connect_retain_flag\",\n",
    "    \"mqtt_connect_username_flag\",\n",
    "    \"mqtt_connect_will_flag\",\n",
    "    \"mqtt_connect_flags\",\n",
    "    \"mqtt_duplicate_flag\",\n",
    "    \"mqtt_header_flags\",\n",
    "    \"mqtt_keep_alive_interval\",\n",
    "    \"mqtt_length\",\n",
    "    \"mqtt_message\",\n",
    "    \"mqtt_message_id\",\n",
    "    \"mqtt_message_type\",\n",
    "    \"mqtt_protocol_length\",\n",
    "    \"mqtt_protocol_name\",\n",
    "    \"mqtt_qos_level\",\n",
    "    \"mqtt_retain_flag\",\n",
    "    \"mqtt_subscription_qos_level\",\n",
    "    \"mqtt_subscription_ack_qos_level\",\n",
    "    \"mqtt_version\",\n",
    "    \"mqtt_will_message\",\n",
    "    \"mqtt_will_message_length\",\n",
    "    \"mqtt_will_topic\",\n",
    "    \"mqtt_will_topic_length\",\n",
    "    \"target_class\",\n",
    "    \"dataset_type\"\n",
    "]\n",
    "\n",
    "df = df.toDF(*new_column_names)\n",
    "\n",
    "train_df = train_df.toDF(*new_column_names)\n",
    "test_df = test_df.toDF(*new_column_names)\n",
    "train_df = train_df.drop(\"dataset_type\")\n",
    "test_df = test_df.drop(\"dataset_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "col_names = new_column_names\n",
    "    # Define the columns\n",
    "binary_cols = [\n",
    "    \"mqtt_connack_reserved_flags\",\n",
    "    \"mqtt_connack_session_present\",\n",
    "    \"mqtt_connect_clean_session_flag\",\n",
    "    \"mqtt_connect_password_flag\",\n",
    "    \"mqtt_connect_reserved_flag\",\n",
    "    \"mqtt_connect_retain_flag\",\n",
    "    \"mqtt_connect_username_flag\",\n",
    "    \"mqtt_connect_will_flag\",\n",
    "    \"mqtt_duplicate_flag\",\n",
    "    \"mqtt_qos_level\",\n",
    "    \"mqtt_retain_flag\",\n",
    "    \"mqtt_connect_qos_level\"\n",
    "]\n",
    "\n",
    "nominal_cols = [\n",
    "    \"tcp_flags\",\n",
    "    \"mqtt_connack_flags\",\n",
    "    \"mqtt_connack_return_code\",\n",
    "    \"mqtt_connect_flags\",\n",
    "    \"mqtt_header_flags\",\n",
    "    \"mqtt_message_type\",\n",
    "    \n",
    "    'mqtt_protocol_name'\n",
    "]\n",
    "\n",
    "continuous_cols = [\n",
    "    \"tcp_time_delta\",\n",
    "    \"tcp_length\",\n",
    "    \"mqtt_keep_alive_interval\",\n",
    "    \"mqtt_length\",\n",
    "    \"mqtt_protocol_length\",\n",
    "    \"mqtt_will_message_length\",\n",
    "    \"mqtt_will_topic_length\",\n",
    "    \"mqtt_will_topic\",\n",
    "    \"mqtt_will_message\",\n",
    "    \"mqtt_subscription_ack_qos_level\",\n",
    "    \"mqtt_subscription_qos_level\",\n",
    "    \"mqtt_message_id\",\n",
    "    \"mqtt_message\"\n",
    "]\n",
    "\n",
    "corelated_cols_to_remove = [\"mqtt_connack_reserved_flags\", \"mqtt_connack_session_present\", \"mqtt_connect_qos_level\", ## zero values\n",
    "                            \"mqtt_connect_reserved_flag\", \"mqtt_connect_retain_flag\", \"mqtt_connect_will_flag\", \"mqtt_message\", \n",
    "                            \"mqtt_subscription_qos_level\", \"mqtt_subscription_ack_qos_level\", \"mqtt_will_message\", \"mqtt_version\",\n",
    "                            \"mqtt_will_message_length\", \"mqtt_will_topic\", \"mqtt_will_topic_length\",'mqtt_protocol_name', \"mqtt_message_id\",\n",
    "\n",
    "\n",
    "                            \"mqtt_protool_length\", \"mqtt_keep_alive_interval\"] #high correlation (1.00), (0.77)\n",
    "\n",
    "class OutcomeCreater(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_multiclass = udf(lambda name:\n",
    "                                  0.0 if name == 'legitimate' else\n",
    "                                  1.0 if name == 'slowite' else\n",
    "                                  2.0 if name == 'bruteforce' else\n",
    "                                  3.0 if name == 'flood' else\n",
    "                                  4.0 if name == 'malformed' else\n",
    "                                  5.0 if name == 'dos' else\n",
    "                                  -1.0, DoubleType())\n",
    "        output_df = dataset.withColumn('outcome', label_to_multiclass(col('target_class')))\n",
    "        output_df = output_df.drop(\"target_class\")\n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class FeatureTypeCaster(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset: DataFrame):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "        return output_df\n",
    "    \n",
    "class ColumnDropper(Transformer):\n",
    "    def __init__(self, columns_to_drop=None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols=nominal_cols, outputCols=nominal_id_cols)\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "    \n",
    "    feature_cols = [col for col in continuous_cols + binary_cols + nominal_onehot_cols if col not in corelated_cols_to_remove]\n",
    "\n",
    "\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "    stage_scaler = StandardScaler(inputCol='vectorized_features', outputCol='features')\n",
    "    stage_outcome = OutcomeCreater()\n",
    "    columns_to_drop = (\n",
    "        nominal_cols + nominal_id_cols + nominal_onehot_cols +\n",
    "        binary_cols + continuous_cols + [\"vectorized_features\"] + corelated_cols_to_remove\n",
    "    )\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop=columns_to_drop)\n",
    "\n",
    "    pipeline = Pipeline(stages=[stage_typecaster, stage_nominal_indexer, stage_nominal_onehot_encoder,\n",
    "                                stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper])\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing pipeline\n",
    "pipeline = get_preprocess_pipeline()\n",
    "\n",
    "# Fit the pipeline to data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transform the data\n",
    "train_df_preprocessed = pipeline_model.transform(train_df)\n",
    "test_df_preprocessed = pipeline_model.transform(test_df)\n",
    "\n",
    "# # Apply the outlier handling function\n",
    "# train_df_with_outlier_handling = find_outliers(train_df_preprocessed)\n",
    "\n",
    "# # Count rows with 4 or fewer outliers\n",
    "# count_4_or_less_outliers = train_df_with_outlier_handling.filter(train_df_with_outlier_handling['total_outliers'] >= 4).count()\n",
    "\n",
    "# print(f\"Number of rows with 4 or fewer outliers: {count_4_or_less_outliers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('features', 'vector'), ('outcome', 'double')]\n"
     ]
    }
   ],
   "source": [
    "print(train_df_preprocessed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the dataframes\n",
    "train_df_final = train_df_preprocessed.cache()\n",
    "test_df_final = test_df_preprocessed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_final = train_df_preprocessed\n",
    "test_df_final = test_df_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "train_df_final_slowmite = train_df_final.filter(col(\"outcome\") == \"1.0\").count()\n",
    "train_df_zeros = train_df_final.filter(F.col(\"outcome\") == \"0.0\").sample(False, train_df_final_slowmite / train_df_final.filter(F.col(\"outcome\") == \"0.0\").count())\n",
    "train_df_final = train_df_final.filter(F.col(\"outcome\") != \"0.0\")\n",
    "train_df_final = train_df_final.union(train_df_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8399438\n"
     ]
    }
   ],
   "source": [
    "print(train_df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Model...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8335.cache.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\r\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:278)\r\n\tat org.apache.spark.sql.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:1255)\r\n\tat org.apache.spark.sql.execution.CacheManager.getOrCloneSessionWithConfigsOff(CacheManager.scala:406)\r\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:121)\r\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)\r\n\tat org.apache.spark.sql.Dataset.persist(Dataset.scala:3775)\r\n\tat org.apache.spark.sql.Dataset.cache(Dataset.scala:3785)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     21\u001b[0m rf_cv \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mrf,\n\u001b[0;32m     22\u001b[0m                        estimatorParamMaps\u001b[38;5;241m=\u001b[39mrf_paramGrid,\n\u001b[0;32m     23\u001b[0m                        evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[0;32m     24\u001b[0m                        numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Random Forest Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Random Forest Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m rf_predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mtransform(test_df_final)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\tuning.py:840\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    838\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kFold(dataset)\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nFolds):\n\u001b[1;32m--> 840\u001b[0m     validation \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m     train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m    843\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m    844\u001b[0m         inheritable_thread_target,\n\u001b[0;32m    845\u001b[0m         _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[0;32m    846\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:1522\u001b[0m, in \u001b[0;36mDataFrame.cache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK_DESER`).\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m \n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;124;03m+- InMemoryTableScan ...\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1522\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8335.cache.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:1570)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \r\n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\r\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\r\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:278)\r\n\tat org.apache.spark.sql.SparkSession$.getOrCloneSessionWithConfigsOff(SparkSession.scala:1255)\r\n\tat org.apache.spark.sql.execution.CacheManager.getOrCloneSessionWithConfigsOff(CacheManager.scala:406)\r\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:121)\r\n\tat org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:93)\r\n\tat org.apache.spark.sql.Dataset.persist(Dataset.scala:3775)\r\n\tat org.apache.spark.sql.Dataset.cache(Dataset.scala:3785)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Classifier 2: Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"outcome\", numTrees=20)\n",
    "\n",
    "# rf_paramGrid = (ParamGridBuilder()\n",
    "#                 .addGrid(rf.numTrees, [10, 20, 40])\n",
    "#                 .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "#                 .build())\n",
    "\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(rf.numTrees, [10, 20])  # Reduced options\n",
    "                .addGrid(rf.maxDepth, [5, 10])   # Reduced options\n",
    "                .build())\n",
    "\n",
    "# Set up CrossValidator for Random Forest\n",
    "rf_cv = CrossValidator(estimator=rf,\n",
    "                       estimatorParamMaps=rf_paramGrid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=3)\n",
    "\n",
    "print(\"Training Random Forest Model...\")\n",
    "rf_model = rf_cv.fit(train_df_final)\n",
    "\n",
    "print(\"Evaluating Random Forest Model...\")\n",
    "rf_predictions = rf_model.transform(test_df_final)\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Test Accuracy of Random Forest Model: {rf_accuracy}\")\n",
    "\n",
    "best_rf_model = rf_model.bestModel\n",
    "\n",
    "print(\"Best Random Forest Model Parameters:\")\n",
    "print(f\"NumTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"MaxDepth: {best_rf_model.getMaxDepth()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression Model...\n",
      "Evaluating Logistic Regression Model...\n",
      "Test Accuracy of Logistic Regression Model: 0.6043538333333334\n",
      "Best Logistic Regression Model Parameters:\n",
      "RegParam: 0.01\n",
      "MaxIter: 10\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Define two classifiers\n",
    "# Classifier 1: Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"outcome\", maxIter=10)\n",
    "\n",
    "\n",
    "\n",
    "# Set up parameter grids for tuning\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "                .addGrid(lr.maxIter, [1, 5, 10])\n",
    "                .build())\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"outcome\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Set up CrossValidator for Logistic Regression\n",
    "lr_cv = CrossValidator(estimator=lr,\n",
    "                       estimatorParamMaps=lr_paramGrid,\n",
    "                       evaluator=evaluator,\n",
    "                       numFolds=3)\n",
    "\n",
    "# Fit models\n",
    "print(\"Training Logistic Regression Model...\")\n",
    "lr_model = lr_cv.fit(train_df_final)\n",
    "\n",
    "\n",
    "# Evaluate on test data\n",
    "print(\"Evaluating Logistic Regression Model...\")\n",
    "lr_predictions = lr_model.transform(test_df_final)\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Test Accuracy of Logistic Regression Model: {lr_accuracy}\")\n",
    "\n",
    "\n",
    "# Record the best models and parameters\n",
    "best_lr_model = lr_model.bestModel\n",
    "\n",
    "print(\"Best Logistic Regression Model Parameters:\")\n",
    "print(f\"RegParam: {best_lr_model._java_obj.getRegParam()}\")\n",
    "print(f\"MaxIter: {best_lr_model._java_obj.getMaxIter()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8318.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda/0x0000029f58039338.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000029f57f29148.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4150)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f583cf9d8.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f57c36c28.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f57b4e810.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000029f57b51fe0.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000029f57b4ead8.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m to_array \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;28;01mlambda\u001b[39;00m v: v\u001b[38;5;241m.\u001b[39mtoArray()\u001b[38;5;241m.\u001b[39mtolist(), ArrayType(FloatType())) \u001b[38;5;66;03m# keep as is\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_df_final, df_validate_final \u001b[38;5;241m=\u001b[39m train_df_final\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m df_train_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m df_validate_pandas \u001b[38;5;241m=\u001b[39m df_validate_final\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, to_array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     14\u001b[0m df_test_pandas \u001b[38;5;241m=\u001b[39m test_df_final\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, to_array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8318.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\r\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda/0x0000029f58039338.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda/0x0000029f57f29148.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4150)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f583cf9d8.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f57c36c28.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.Dataset$$Lambda/0x0000029f57b4e810.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000029f57b51fe0.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda/0x0000029f57b4ead8.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "to_array = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType())) # keep as is\n",
    "\n",
    "train_df_final, df_validate_final = train_df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "df_train_pandas = train_df_final.withColumn('features', to_array('features')).toPandas()\n",
    "df_validate_pandas = df_validate_final.withColumn('features', to_array('features')).toPandas()\n",
    "df_test_pandas = test_df_final.withColumn('features', to_array('features')).toPandas()\n",
    "\n",
    "x_train = torch.from_numpy(np.array(df_train_pandas['features'].values.tolist(),np.float32))\n",
    "y_train = torch.from_numpy(np.array(df_train_pandas['outcome'].values.tolist(),np.int64))\n",
    "\n",
    "x_validate = torch.from_numpy(np.array(df_validate_pandas['features'].values.tolist(),np.float32))\n",
    "y_validate = torch.from_numpy(np.array(df_validate_pandas['outcome'].values.tolist(),np.int64))\n",
    "\n",
    "x_test = torch.from_numpy(np.array(df_test_pandas['features'].values.tolist(),np.float32))\n",
    "y_test = torch.from_numpy(np.array(df_test_pandas['outcome'].values.tolist(),np.int64))\n",
    "\n",
    "class MyDataset(Dataset): \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "validate_dataset = MyDataset(x_validate, y_validate)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sequential(x)\n",
    "        return y\n",
    "    \n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sequential(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += y_batch.size(0)\n",
    "                correct_val += (predicted == y_batch).sum().item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    running_test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            correct_test += (predicted == y_batch).sum().item()\n",
    "\n",
    "    test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[1]\n",
    "output_dim = len(torch.unique(y_train))\n",
    "\n",
    "# Hyperparameter options\n",
    "learning_rates = [0.01, 0.001]\n",
    "num_epochs_list = [10, 20]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "best_val_accuracy_shallow = 0.0\n",
    "best_params_shallow = {}\n",
    "best_test_accuracy_shallow = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_epochs in num_epochs_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTraining ShallowMLP with lr={lr}, num_epochs={num_epochs}, batch_size={batch_size}\")\n",
    "            \n",
    "            # Update DataLoaders with new batch_size\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            # Initialize model, criterion, and optimizer\n",
    "            model = ShallowMLP(input_dim, output_dim)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Train model using the validation set\n",
    "            model, val_accuracy = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "            \n",
    "            # Evaluate the model on the test set\n",
    "            test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "            \n",
    "            # Record best model based on validation accuracy\n",
    "            if val_accuracy > best_val_accuracy_shallow:\n",
    "                best_val_accuracy_shallow = val_accuracy\n",
    "                best_params_shallow = {'lr': lr, 'num_epochs': num_epochs, 'batch_size': batch_size}\n",
    "                best_model_shallow = model.state_dict()\n",
    "                best_test_accuracy_shallow = test_accuracy  # Save corresponding test accuracy\n",
    "\n",
    "print(\"\\nBest ShallowMLP Validation Accuracy: {:.4f}\".format(best_val_accuracy_shallow))\n",
    "print(\"Corresponding Test Accuracy: {:.4f}\".format(best_test_accuracy_shallow))\n",
    "print(\"Best Hyperparameters for ShallowMLP:\")\n",
    "print(best_params_shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy_deep = 0.0\n",
    "best_params_deep = {}\n",
    "best_test_accuracy_deep = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_epochs in num_epochs_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTraining DeepMLP with lr={lr}, num_epochs={num_epochs}, batch_size={batch_size}\")\n",
    "\n",
    "            # Update DataLoaders with new batch_size\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize model, criterion, and optimizer\n",
    "            model = DeepMLP(input_dim, output_dim)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Train model\n",
    "            model, val_accuracy = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "            # Record best model based on validation accuracy\n",
    "            if val_accuracy > best_val_accuracy_deep:\n",
    "                best_val_accuracy_deep = val_accuracy\n",
    "                best_params_deep = {'lr': lr, 'num_epochs': num_epochs, 'batch_size': batch_size}\n",
    "                best_model_deep_state = model.state_dict()\n",
    "                best_test_accuracy_deep = test_accuracy\n",
    "\n",
    "print(\"\\nBest DeepMLP Validation Accuracy: {:.4f}\".format(best_val_accuracy_deep))\n",
    "print(\"Corresponding Test Accuracy: {:.4f}\".format(best_test_accuracy_deep))\n",
    "print(\"Best Hyperparameters for DeepMLP:\")\n",
    "print(best_params_deep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
